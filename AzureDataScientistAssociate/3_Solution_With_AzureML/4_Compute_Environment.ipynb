{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Environment and Compute targets\n",
    "\n",
    "The runtime context for each experiment run consists of two elements:\n",
    "- The _environment_\n",
    "- The _compute target_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Environments\n",
    "\n",
    "Python runs are based on _virtual environments_ in which both the python version and packages are defined. In most, Python installation, packages are installed and managed by Conda or Pip.\n",
    "\n",
    "To improve portability, we usually create environment in docker containers hosted in compute targets.\n",
    "\n",
    "In general, AzureML handles the creation of this environment through the docker container and manages packages and python. It encapsulates the environment in the _Environment_ class which can be registered in the workspace."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating an environment from a specification file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment(name = 'training_environment',\n",
    "                  file_path = 'environment.yml')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating an environment from an existing Conda environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_13904\\2355583178.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m env = Environment.from_existing_conda_environment(name='azure_env',\n\u001B[1;32m----> 2\u001B[1;33m                                                   conda_environment_name='training_environment')\n\u001B[0m",
      "\u001B[1;32m~\\Anaconda3\\envs\\azure_env\\lib\\site-packages\\azureml\\core\\environment.py\u001B[0m in \u001B[0;36mfrom_existing_conda_environment\u001B[1;34m(name, conda_environment_name)\u001B[0m\n\u001B[0;32m   1326\u001B[0m         \"\"\"  # noqa: E501\n\u001B[0;32m   1327\u001B[0m         \u001B[0menv\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1328\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[0mEnvironment\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_conda_installation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mEnvironment\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_conda_env_existance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconda_environment_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1329\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1330\u001B[0m                 print(\"Exporting conda specifications for \"\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\azure_env\\lib\\site-packages\\azureml\\core\\environment.py\u001B[0m in \u001B[0;36m_check_conda_installation\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1630\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1631\u001B[0m             \u001B[0mconda_version_cmd\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m\"conda\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"--version\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1632\u001B[1;33m             \u001B[0mconda_version\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msubprocess\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheck_output\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconda_version_cmd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"UTF-8\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"conda\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1633\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1634\u001B[0m             \u001B[1;32mfrom\u001B[0m \u001B[0mdistutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mversion\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLooseVersion\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\azure_env\\lib\\subprocess.py\u001B[0m in \u001B[0;36mcheck_output\u001B[1;34m(timeout, *popenargs, **kwargs)\u001B[0m\n\u001B[0;32m    409\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    410\u001B[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001B[1;32m--> 411\u001B[1;33m                **kwargs).stdout\n\u001B[0m\u001B[0;32m    412\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    413\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\azure_env\\lib\\subprocess.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[0;32m    486\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'stderr'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mPIPE\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    487\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 488\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mPopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mpopenargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mprocess\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    489\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    490\u001B[0m             \u001B[0mstdout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstderr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprocess\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcommunicate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\azure_env\\lib\\subprocess.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001B[0m\n\u001B[0;32m    798\u001B[0m                                 \u001B[0mc2pread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mc2pwrite\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    799\u001B[0m                                 \u001B[0merrread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merrwrite\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 800\u001B[1;33m                                 restore_signals, start_new_session)\n\u001B[0m\u001B[0;32m    801\u001B[0m         \u001B[1;32mexcept\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    802\u001B[0m             \u001B[1;31m# Cleanup if the child failed starting.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\azure_env\\lib\\subprocess.py\u001B[0m in \u001B[0;36m_execute_child\u001B[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001B[0m\n\u001B[0;32m   1205\u001B[0m                                          \u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1206\u001B[0m                                          \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfspath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcwd\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mcwd\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1207\u001B[1;33m                                          startupinfo)\n\u001B[0m\u001B[0;32m   1208\u001B[0m             \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1209\u001B[0m                 \u001B[1;31m# Child is launched. Close the parent's copy of those pipe\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "env = Environment.from_existing_conda_environment(name='azure_env',\n",
    "                                                  conda_environment_name='azure_env')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating an environment by specifying packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "env = Environment('training_environment')\n",
    "deps = CondaDependencies.create(conda_packages = ['scikit-learn','pandas','numpy'],\n",
    "                                pip_packages=['azureml-defaults'])\n",
    "env.python.conda_dependencies = deps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Experiments usually use docker containers to create envs, otherwise the environment would be created in computer target (use_docker = False)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "docker_config = DockerConfiguration(use_docker=True)\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='Script',\n",
    "                                script='1_Script.py',\n",
    "                                environment=env,\n",
    "                                docker_runtime_config=docker_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "AzureML has multiple docker's images which are chosen based on the type of compute target: indeed, if you are using a compute target with GPU, a docker's image with CUDA will be chosen.\n",
    "Moreover, it is possible to create and register your own docker image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# env.docker.base_image='my-base-image'\n",
    "# env.docker.base_image_registry='myregistry.azurecr.io/myimage'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the other hand, you can add some additional layers based on the default image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# env.docker.base_image=None\n",
    "# env.docker.base_image_registry='myregistry.azurecr.io/myimage'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If your image includes the dependencies, you can flag it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# env.python.user_managed_dependencies=True\n",
    "# env.python.interpreter_path = '/opt/miniconda/bin/python'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Environment can be registered."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{\n    \"assetId\": \"azureml://locations/francecentral/workspaces/0ee9671f-7144-4125-a665-a1b9cffe5395/environments/training_environment/versions/1\",\n    \"databricks\": {\n        \"eggLibraries\": [],\n        \"jarLibraries\": [],\n        \"mavenLibraries\": [],\n        \"pypiLibraries\": [],\n        \"rcranLibraries\": []\n    },\n    \"docker\": {\n        \"arguments\": [],\n        \"baseDockerfile\": null,\n        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20221101.v1\",\n        \"baseImageRegistry\": {\n            \"address\": null,\n            \"password\": null,\n            \"registryIdentity\": null,\n            \"username\": null\n        },\n        \"buildContext\": null,\n        \"enabled\": false,\n        \"platform\": {\n            \"architecture\": \"amd64\",\n            \"os\": \"Linux\"\n        },\n        \"sharedVolumes\": true,\n        \"shmSize\": null\n    },\n    \"environmentVariables\": {\n        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n    },\n    \"inferencingStackVersion\": null,\n    \"name\": \"training_environment\",\n    \"python\": {\n        \"baseCondaEnvironment\": null,\n        \"condaDependencies\": {\n            \"channels\": [\n                \"anaconda\",\n                \"conda-forge\"\n            ],\n            \"dependencies\": [\n                \"python=3.8.13\",\n                {\n                    \"pip\": [\n                        \"azureml-defaults\"\n                    ]\n                }\n            ],\n            \"name\": \"project_environment\"\n        },\n        \"condaDependenciesFile\": null,\n        \"interpreterPath\": \"python\",\n        \"userManagedDependencies\": false\n    },\n    \"r\": null,\n    \"spark\": {\n        \"packages\": [],\n        \"precachePackages\": true,\n        \"repositories\": []\n    },\n    \"version\": \"1\"\n}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "env.register(workspace=ws)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: training_environment\n",
      "Name: AzureML-VowpalWabbit-8.8.0\n",
      "Name: AzureML-PyTorch-1.3-CPU\n",
      "Name: AzureML-Dask-CPU\n",
      "Name: AzureML-Dask-GPU\n",
      "Name: AzureML-Triton\n",
      "Name: AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu\n",
      "Name: AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\n",
      "Name: AzureML-sklearn-1.0-ubuntu20.04-py38-cpu\n",
      "Name: AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11-gpu\n",
      "Name: AzureML-tensorflow-2.6-ubuntu20.04-py38-cuda11-gpu\n",
      "Name: AzureML-tensorflow-2.5-ubuntu20.04-py38-cuda11-gpu\n",
      "Name: AzureML-tensorflow-2.7-ubuntu20.04-py38-cuda11-gpu\n",
      "Name: AzureML-pytorch-1.7-ubuntu18.04-py37-cuda11-gpu\n",
      "Name: AzureML-pytorch-1.8-ubuntu18.04-py37-cuda11-gpu\n",
      "Name: AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu\n",
      "Name: AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu\n",
      "Name: AzureML-minimal-ubuntu18.04-py37-cpu-inference\n",
      "Name: AzureML-responsibleai-0.20-ubuntu20.04-py38-cpu\n",
      "Name: AzureML-responsibleai-0.21-ubuntu20.04-py38-cpu\n",
      "Name: AzureML-PTA-pytorch-1.11-py38-cuda11.3-gpu\n",
      "Name: AzureML-PTA-pytorch-1.11-py38-cuda11.5-gpu\n",
      "Name: AzureML-ACPT-pytorch-1.12-py38-cuda11.6-gpu\n",
      "Name: AzureML-ACPT-pytorch-1.12-py39-cuda11.6-gpu\n",
      "Name: AzureML-ACPT-pytorch-1.11-py38-cuda11.5-gpu\n",
      "Name: AzureML-ACPT-pytorch-1.11-py38-cuda11.3-gpu\n",
      "Name: AzureML-minimal-ubuntu18.04-py37-cuda11.0.3-gpu-inference\n"
     ]
    }
   ],
   "source": [
    "for env_name in Environment.list(workspace = ws):\n",
    "    print('Name:', env_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Retrieve a registered environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are trying to retrieve an environment which doesn't exist, AzureML will create it for you."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "training_env = Environment.get(workspace=ws, name='training_environment')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute\n",
    "\n",
    "In Azure machine learning, _Compute Targets_ are real or virtual computers in which experiments are run.\n",
    "\n",
    "There are multiple choices of compute target, such as the following ones:\n",
    "- Local Compute: the computers on which the code which initiate the experiment is written. It can be your local compute or a virtual machine when you are using the notebook asset in AzureML.\n",
    "- Compute cluster: AzureML gives you the possibility to use multi-node of Virtual Machines that scales up or down to meet the demand. It can be useful when a requirement is the scalability or some kind of parallel processing should be used.\n",
    "- Attached compute: If you already use an Azure-based compute environment for data science, such as a virtual machine or an Azure Databricks cluster, you can attach it to your Azure Machine Learning workspace and use it as a compute target for certain types of workload.\n",
    "- Inference clusters: this kind of compute represents an Azure Kubernetes Service cluster and can only be used to deploy trained models as inferencing services.\n",
    "\n",
    "Using multiple Compute targets can have multiple advantages:\n",
    "- Develop and test your code on a low-cost machine before using a cluster for deployment.\n",
    "- Choosing the best compute for the current task. For instance, it could be more effective to use a compute target with a performing GPU for training a DNN and then, switch to a CPU-based compute target.\n",
    "\n",
    "Moreover, compute target costs depends on what resources are you exploiting. So, you can scale automatically based on the workload needed and stop the processes automatically as well.\n",
    "\n",
    "A compute target can be created through AzureML studio or by SDK."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A _managed_ compute target is one that is managed by AzureML, such as a cluster."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We would like to create a cluster based on the image of the virtual machine STANDARD_DS11_V2 up to four nodes.\n",
    "\n",
    "The _dedicated_ priority means that the current machine is reserved for us, otherwise if it was equal to lowpriority, we would lose our priority if someone with a higher priority asked for that."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InProgress.\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "compute_name = 'aml-cluster'\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2',\n",
    "                                                       min_nodes=0,\n",
    "                                                       max_nodes=4,\n",
    "                                                       vm_priority='dedicated')\n",
    "\n",
    "aml_cluster = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "aml_cluster.wait_for_completion(show_output=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An _unmanaged_ compute target is one that is not managed by AzureML, such as an Azure virtual machine or Azure Databricks cluster.\n",
    "\n",
    "In the code below, a databricks cluster will be attached to the personal workspace."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Specify a name for the compute (unique within the workspace)\n",
    "# compute_name = 'db_cluster'\n",
    "\n",
    "# Define configuration for existing Azure Databricks cluster\n",
    "# db_workspace_name = 'db_workspace'\n",
    "# db_resource_group = 'db_resource_group'\n",
    "# db_access_token = '1234-abc-5678-defg-90...'\n",
    "# db_config = DatabricksCompute.attach_configuration(resource_group=db_resource_group,\n",
    "#                                                   workspace_name=db_workspace_name,\n",
    "#                                                   access_token=db_access_token)\n",
    "# Create the compute\n",
    "# databricks_compute = ComputeTarget.attach(ws, compute_name, db_config)\n",
    "# databricks_compute.wait_for_completion(True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if a compute target already exists."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster not found.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "compute_name = \"abl-cluster\"\n",
    "\n",
    "try:\n",
    "    aml_cluster = ComputeTarget(workspace=ws, name=compute_name)\n",
    "    print('Found existing cluster.')\n",
    "except ComputeTargetException:\n",
    "    print('Cluster not found.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use a particular compute target when running an experiment, you should specify its name in the ScriptRunConfig.\n",
    "\n",
    "First of all, the compute target is started and the environment is created, then the job can start."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from azureml.core import Environment, ScriptRunConfig\n",
    "\n",
    "compute_name = 'aml-cluster'\n",
    "\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='Script',\n",
    "                                script='1_Script.py',\n",
    "                                environment=training_env,\n",
    "                                compute_target=compute_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is also possible to specify the compute target object."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "cluster = ComputeTarget(workspace=ws, name=compute_name)\n",
    "\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='Script',\n",
    "                                script='1_Script.py',\n",
    "                                environment=training_env,\n",
    "                                compute_target=cluster)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Retrieve data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Uploading file to diabetes-data\n",
      "Uploading an estimated of 1 files\n",
      "Target already exists. Skipping upload for diabetes-data\\diabetes.csv\n",
      "Uploaded 0 files\n",
      "Creating new dataset\n",
      "Dataset registered.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "default_ds = ws.get_default_datastore()\n",
    "name_data = 'diabetes dataset'\n",
    "\n",
    "if name_data not in default_ds.name:\n",
    "    Dataset.File.upload_directory(src_dir='./Script/data',\n",
    "                                  target=DataPath(default_ds, 'diabetes-data'))\n",
    "\n",
    "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
    "\n",
    "    try:\n",
    "        tab_data_set = tab_data_set.register(workspace=ws,\n",
    "                                name='diabetes dataset',\n",
    "                                description='diabetes data',\n",
    "                                tags = {'format':'CSV'},\n",
    "                                create_new_version=True)\n",
    "        print('Dataset registered.')\n",
    "\n",
    "    except Exception as ex:\n",
    "\n",
    "        print(ex)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "AzureML creates a standard env if you don't define another configuration. Especially, it cosists of azureml-defaults and numpy and pandas.\n",
    "\n",
    "In the configuration file, the conda dependencies are defined after the pip dependencies and so, the conda dep should contains also pip.\n",
    "\n",
    "If you reuse an environment, the first run cache it which will be retrived quickier.\n",
    "\n",
    "You cannot use the prefix AzureML for your own envs because it is reserved for the curated envs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment.from_conda_specification('experiment-env', 'environment.yml')\n",
    "# Let Azure ML manage dependencies\n",
    "# env.python.user_managed_dependencies = False\n",
    "\n",
    "env.register(workspace=ws)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "data = ws.datasets.get(name_data)\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory='Script',\n",
    "                                script='5_Train_Dataset.py',\n",
    "                                arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n",
    "                                             '--input-data', data.as_named_input('training_data')], # Reference to dataset\n",
    "                                environment=env,\n",
    "                                compute_target='ravazzil-compute') # Use docker to host environment\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be3aadc66ceb4bc0a9829196834fe3b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# submit the experiment\n",
    "experiment_name = 'mslearn-train-diabetes'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "outputs/5_Train_with_parameters_and_dataset.py\n",
      "outputs/diabetes_model.pkl\n",
      "outputs/my_diabetes_model.pkl\n",
      "system_logs/cs_capability/cs-capability.log\n",
      "system_logs/hosttools_capability/hosttools-capability.log\n",
      "system_logs/lifecycler/lifecycler.log\n",
      "system_logs/lifecycler/vm-bootstrapper.log\n",
      "system_logs/metrics_capability/metrics-capability.log\n",
      "system_logs/snapshot_capability/snapshot-capability.log\n"
     ]
    }
   ],
   "source": [
    "metrics = run.get_metrics()\n",
    "for key in metrics.keys():\n",
    "        print(key, metrics.get(key))\n",
    "print('\\n')\n",
    "for file in run.get_file_names():\n",
    "    print(file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InProgress.\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "compute_name = 'aml-cluster'\n",
    "\n",
    "if compute_name not in ws.compute_targets:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2',\n",
    "                                                       min_nodes=0,\n",
    "                                                       max_nodes=2)\n",
    "\n",
    "    aml_cluster = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "    aml_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "    print('Compute target successfully created.')\n",
    "\n",
    "else:\n",
    "    print('Compute target already created.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory='Script',\n",
    "                                script='5_Train_Dataset.py',\n",
    "                                arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n",
    "                                             '--input-data', data.as_named_input('training_data')], # Reference to dataset\n",
    "                                environment=env,\n",
    "                                compute_target=compute_name) # Use docker to host environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c268f7bd03748ed81401e26fd88c2f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# submit the experiment\n",
    "experiment_name = 'mslearn-train-diabetes'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "It takes a while in order to run all the experiment because first of all, the container image must be built with the conda environment, then the node of the cluster mst be started and the image must be deployed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steady 1\n"
     ]
    }
   ],
   "source": [
    "cluster_state = aml_cluster.get_status()\n",
    "print(cluster_state.allocation_state, cluster_state.current_node_count)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# It blocks the kernel for the local notebook.\n",
    "run.wait_for_completion()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
