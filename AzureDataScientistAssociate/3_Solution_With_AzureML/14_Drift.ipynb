{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Data drift\n",
    "\n",
    "You typically train a machine learning model using a historical dataset that is representative of the new data that your model will receive for inferencing. However, over time there may be trends that change the profile of the data, making your model less accurate.\n",
    "\n",
    "Azure Machine Learning supports data drift monitoring through the use of datasets. You can capture new feature data in a dataset and compare it to the dataset with which the model was trained.\n",
    "\n",
    "To monitor data drift using registered datasets, you need to register two datasets:\n",
    "\n",
    "- A baseline dataset - usually the original training data.\n",
    "- A target dataset that will be compared to the baseline based on time intervals. This dataset requires a column for each feature you want to compare, and a timestamp column so the rate of data drift can be measured.\n",
    "\n",
    "You can schedule when the data drift task should be started and configure the alert along with its threshold. Data drift is measured using a calculated magnitude of change in the statistical distribution of feature values over time. You can expect some natural random variation between the baseline and target datasets, but you should monitor for large changes that might indicate significant data drift."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# !pip install azureml-datadrift"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Uploading file to diabetes-baseline/\n",
      "Uploading an estimated of 8 files\n",
      "Target already exists. Skipping upload for diabetes-baseline/diabetes.csv\n",
      "Target already exists. Skipping upload for diabetes-baseline/diabetes2.csv\n",
      "Target already exists. Skipping upload for diabetes-baseline/diabetes_2022-12-14.csv\n",
      "Target already exists. Skipping upload for diabetes-baseline/diabetes_2022-12-21.csv\n",
      "Target already exists. Skipping upload for diabetes-baseline/diabetes_2022-12-28.csv\n",
      "Target already exists. Skipping upload for diabetes-baseline/diabetes_2023-01-04.csv\n",
      "Target already exists. Skipping upload for diabetes-baseline/diabetes_2023-01-11.csv\n",
      "Target already exists. Skipping upload for diabetes-baseline/diabetes_2023-01-18.csv\n",
      "Uploaded 0 files\n",
      "Creating new dataset\n",
      "Registering baseline dataset...\n",
      "Baseline dataset registered!\n"
     ]
    }
   ],
   "source": [
    "# Baseline dataset.\n",
    "\n",
    "from azureml.core import Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "# Upload the baseline data\n",
    "default_ds = ws.get_default_datastore()\n",
    "Dataset.File.upload_directory(src_dir='Script/data/',\n",
    "                              target=DataPath(default_ds, 'diabetes-baseline/')\n",
    "                              )\n",
    "\n",
    "# Create and register the baseline dataset\n",
    "print('Registering baseline dataset...')\n",
    "\n",
    "baseline_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-baseline/*.csv'))\n",
    "baseline_data_set = baseline_data_set.register(workspace=ws,\n",
    "                           name='diabetes baseline',\n",
    "                           description='diabetes baseline data',\n",
    "                           tags = {'format':'CSV'},\n",
    "                           create_new_version=True)\n",
    "\n",
    "print('Baseline dataset registered!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating simulated data...\n",
      "save ./Script/data/diabetes_2022-12-14.csv\n",
      "save ./Script/data/diabetes_2022-12-21.csv\n",
      "save ./Script/data/diabetes_2022-12-28.csv\n",
      "save ./Script/data/diabetes_2023-01-04.csv\n",
      "save ./Script/data/diabetes_2023-01-11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"datastore.upload_files\" is deprecated after version 1.0.69. Please use \"FileDatasetFactory.upload_directory\" instead. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save ./Script/data/diabetes_2023-01-18.csv\n",
      "Uploading an estimated of 1 files\n",
      "Uploading ./Script/data/diabetes_2022-12-14.csv\n",
      "Uploaded ./Script/data/diabetes_2022-12-14.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Uploading an estimated of 1 files\n",
      "Uploading ./Script/data/diabetes_2022-12-21.csv\n",
      "Uploaded ./Script/data/diabetes_2022-12-21.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Uploading an estimated of 1 files\n",
      "Uploading ./Script/data/diabetes_2022-12-28.csv\n",
      "Uploaded ./Script/data/diabetes_2022-12-28.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Uploading an estimated of 1 files\n",
      "Uploading ./Script/data/diabetes_2023-01-04.csv\n",
      "Uploaded ./Script/data/diabetes_2023-01-04.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Uploading an estimated of 1 files\n",
      "Uploading ./Script/data/diabetes_2023-01-11.csv\n",
      "Uploaded ./Script/data/diabetes_2023-01-11.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Uploading an estimated of 1 files\n",
      "Uploading ./Script/data/diabetes_2023-01-18.csv\n",
      "Uploaded ./Script/data/diabetes_2023-01-18.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    }
   ],
   "source": [
    "# Target dataset.\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "print('Generating simulated data...')\n",
    "\n",
    "# Load the smaller of the two data files\n",
    "data = pd.read_csv('./Script/data/diabetes2.csv')\n",
    "\n",
    "# We'll generate data for the past 6 weeks\n",
    "weeknos = reversed(range(6))\n",
    "\n",
    "file_paths = []\n",
    "for weekno in weeknos:\n",
    "\n",
    "    # Get the date X weeks ago\n",
    "    data_date = dt.date.today() - dt.timedelta(weeks=weekno)\n",
    "\n",
    "    # Modify data to ceate some drift\n",
    "    data['Pregnancies'] = data['Pregnancies'] + 1\n",
    "    data['Age'] = round(data['Age'] * 1.2).astype(int)\n",
    "    data['BMI'] = data['BMI'] * 1.1\n",
    "\n",
    "    # Save the file with the date encoded in the filename\n",
    "    file_path = './Script/data/diabetes_{}.csv'.format(data_date.strftime(\"%Y-%m-%d\"))\n",
    "    print(f'save {file_path}')\n",
    "    data.to_csv(file_path)\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "# Upload the files\n",
    "path_on_datastore = 'diabetes-target'\n",
    "\n",
    "for file in file_paths:\n",
    "    default_ds.upload_files(files = [file],\n",
    "                            target_path=path_on_datastore,\n",
    "                            overwrite=True,\n",
    "                            show_progress=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dataset registered!\n"
     ]
    }
   ],
   "source": [
    "# Use the folder partition format to define a dataset with a 'date' timestamp column\n",
    "# partition_format = path_on_datastore + '/diabetes_{date:yyyy-MM-dd}.csv'\n",
    "\n",
    "target_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, path_on_datastore + '/*.csv'))\n",
    "print('Registering target dataset...')\n",
    "# Register the target dataset\n",
    "\n",
    "target_data_set = target_data_set.with_timestamp_columns('date').register(workspace=ws,\n",
    "                                                                          name='diabetes target',\n",
    "                                                                          description='diabetes target data',\n",
    "                                                                          tags = {'format':'CSV'},\n",
    "                                                                          create_new_version=True)\n",
    "\n",
    "print('Target dataset registered!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InProgress.\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"ravazzil-cluster\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{'_logger': <_TelemetryLoggerContextAdapter azureml.datadrift._logging._telemetry_logger.azureml.datadrift.datadriftdetector (DEBUG)>, '_workspace': Workspace.create(name='ravazzil-workspace', subscription_id='d12c1b85-0a70-4232-b483-12d1ffcfc148', resource_group='ResourceGroupRavazzi'), '_frequency': 'Week', '_schedule_start': None, '_schedule_id': None, '_interval': 1, '_state': 'Disabled', '_alert_config': None, '_type': 'DatasetBased', '_id': '46c9ac0a-2041-4837-8338-8e9a014e46c2', '_compute_target_name': 'ravazzil-cluster', '_drift_threshold': 0.3, '_baseline_dataset_id': 'f41c8b60-7daf-488b-b11e-d29819dae3a2', '_target_dataset_id': '3b81a32d-92d8-4398-bd0a-c5eff6c15949', '_feature_list': ['Pregnancies', 'Age', 'BMI'], '_latency': 24, '_name': 'mslearn-diabates-drift', '_latest_run_time': None, '_client': <azureml.datadrift._restclient.datadrift_client.DataDriftClient object at 0x00000298113CD708>}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.datadrift import DataDriftDetector\n",
    "\n",
    "# set up feature list\n",
    "features = ['Pregnancies', 'Age', 'BMI']\n",
    "\n",
    "# set up data drift detector\n",
    "monitor = DataDriftDetector.create_from_datasets(ws,\n",
    "                                                 'mslearn-diabates-drift',\n",
    "                                                 baseline_data_set,\n",
    "                                                 target_data_set,\n",
    "                                                 compute_target=cluster_name,\n",
    "                                                 frequency='Week',\n",
    "                                                 feature_list=features,\n",
    "                                                 drift_threshold=.3,\n",
    "                                                 latency=24)\n",
    "monitor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0138bd39403e477a8ca64c44275494ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process '/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error:     return func(*args, **kwargs)\\n  File \\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\", line 624, in time_between\\n    return self._time_filter(self.time_between.__name__,\\n  File \\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\", line 867, in _time_filter\\n    self._validate_timestamp_columns([col_fine_timestamp, col_coarse_timestamp])\\n  File \\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\", line 924, in _validate_timestamp_columns\\n    _validate_has_columns(self._dataflow, columns, [FieldType.DATE for c in columns])\\n  File \\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/dataset_error_handling.py\\\", line 81, in _validate_has_columns\\n    raise DatasetValidationError('The specified columns {} do not exist in the current dataset.'\\nazureml.data.dataset_error_handling.DatasetValidationError: DatasetValidationError:\\n\\tMessage: The specified columns ['date'] do not exist in the current dataset.\\n\\tInnerException None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"The specified columns ['date'] do not exist in the current dataset.\\\"\\n    }\\n}\\n\\n\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"componentName\": \"CommonRuntime\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Execution failed. User process '/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error:     return func(*args, **kwargs)\\\\n  File \\\\\\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\\\\\", line 624, in time_between\\\\n    return self._time_filter(self.time_between.__name__,\\\\n  File \\\\\\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\\\\\", line 867, in _time_filter\\\\n    self._validate_timestamp_columns([col_fine_timestamp, col_coarse_timestamp])\\\\n  File \\\\\\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\\\\\", line 924, in _validate_timestamp_columns\\\\n    _validate_has_columns(self._dataflow, columns, [FieldType.DATE for c in columns])\\\\n  File \\\\\\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/dataset_error_handling.py\\\\\\\", line 81, in _validate_has_columns\\\\n    raise DatasetValidationError('The specified columns {} do not exist in the current dataset.'\\\\nazureml.data.dataset_error_handling.DatasetValidationError: DatasetValidationError:\\\\n\\\\tMessage: The specified columns ['date'] do not exist in the current dataset.\\\\n\\\\tInnerException None\\\\n\\\\tErrorResponse \\\\n{\\\\n    \\\\\\\"error\\\\\\\": {\\\\n        \\\\\\\"code\\\\\\\": \\\\\\\"UserError\\\\\\\",\\\\n        \\\\\\\"message\\\\\\\": \\\\\\\"The specified columns ['date'] do not exist in the current dataset.\\\\\\\"\\\\n    }\\\\n}\\\\n\\\\n\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\",\\n    \\\"componentName\\\": \\\"CommonRuntime\\\"\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mActivityFailedException\u001B[0m                   Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8736/4179236900.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mRunDetails\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbackfill\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mbackfill\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwait_for_completion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Anaconda3\\envs\\azure_env\\lib\\site-packages\\azureml\\core\\run.py\u001B[0m in \u001B[0;36mwait_for_completion\u001B[1;34m(self, show_output, wait_post_processing, raise_on_error)\u001B[0m\n\u001B[0;32m    880\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    881\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mraise_on_error\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 882\u001B[1;33m                     \u001B[1;32mraise\u001B[0m \u001B[0mActivityFailedException\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merror_details\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindent\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    883\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    884\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mfinal_details\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mActivityFailedException\u001B[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process '/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error:     return func(*args, **kwargs)\\n  File \\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\", line 624, in time_between\\n    return self._time_filter(self.time_between.__name__,\\n  File \\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\", line 867, in _time_filter\\n    self._validate_timestamp_columns([col_fine_timestamp, col_coarse_timestamp])\\n  File \\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\", line 924, in _validate_timestamp_columns\\n    _validate_has_columns(self._dataflow, columns, [FieldType.DATE for c in columns])\\n  File \\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/dataset_error_handling.py\\\", line 81, in _validate_has_columns\\n    raise DatasetValidationError('The specified columns {} do not exist in the current dataset.'\\nazureml.data.dataset_error_handling.DatasetValidationError: DatasetValidationError:\\n\\tMessage: The specified columns ['date'] do not exist in the current dataset.\\n\\tInnerException None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"The specified columns ['date'] do not exist in the current dataset.\\\"\\n    }\\n}\\n\\n\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"componentName\": \"CommonRuntime\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Execution failed. User process '/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error:     return func(*args, **kwargs)\\\\n  File \\\\\\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\\\\\", line 624, in time_between\\\\n    return self._time_filter(self.time_between.__name__,\\\\n  File \\\\\\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\\\\\", line 867, in _time_filter\\\\n    self._validate_timestamp_columns([col_fine_timestamp, col_coarse_timestamp])\\\\n  File \\\\\\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/tabular_dataset.py\\\\\\\", line 924, in _validate_timestamp_columns\\\\n    _validate_has_columns(self._dataflow, columns, [FieldType.DATE for c in columns])\\\\n  File \\\\\\\"/azureml-envs/azureml_b7f96dea2d17a49b2f9af79a608f13c5/lib/python3.8/site-packages/azureml/data/dataset_error_handling.py\\\\\\\", line 81, in _validate_has_columns\\\\n    raise DatasetValidationError('The specified columns {} do not exist in the current dataset.'\\\\nazureml.data.dataset_error_handling.DatasetValidationError: DatasetValidationError:\\\\n\\\\tMessage: The specified columns ['date'] do not exist in the current dataset.\\\\n\\\\tInnerException None\\\\n\\\\tErrorResponse \\\\n{\\\\n    \\\\\\\"error\\\\\\\": {\\\\n        \\\\\\\"code\\\\\\\": \\\\\\\"UserError\\\\\\\",\\\\n        \\\\\\\"message\\\\\\\": \\\\\\\"The specified columns ['date'] do not exist in the current dataset.\\\\\\\"\\\\n    }\\\\n}\\\\n\\\\n\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\",\\n    \\\"componentName\\\": \\\"CommonRuntime\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "backfill = monitor.backfill(dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
    "\n",
    "RunDetails(backfill).show()\n",
    "backfill.wait_for_completion()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date 2022-12-04\n",
      "end_date 2023-01-22\n",
      "frequency Week\n"
     ]
    }
   ],
   "source": [
    "drift_metrics = backfill.get_metrics()\n",
    "for metric in drift_metrics:\n",
    "    print(metric, drift_metrics[metric])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
