{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##  Deploy real-time machine learning services with Azure Machine Learning\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "_Inferencing_ refers to the ability of a trained model to predict labels fornew data which the model has not been trained. The model is deployed as part of a service which enables applications to request some kind of inference from a small number of data observations.\n",
    "\n",
    "Azure Kubernetes Services (AKS) can be used to deploy the model as a service.\n",
    "\n",
    "AzureML uses containers as deployment mechanism by packaging the model and the code to use it as an image taht can be deployed to a container in your chosen compute target. For testing and development is ok to use your local machine, a compute instances or Azure Container Instances (ACI) but for production, you need to use some compute which meets some requirements related to scalability, performance and security.\n",
    "\n",
    "In order to do so, you should follow these tasks:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "1. After successfully training a model, it should be registered because the real-time inference service will load it when required.\n",
    "\n",
    "<code>\n",
    "from azureml.core import Model\n",
    "\n",
    "classification_model = Model.register(workpace = ws,\n",
    "                                      model_name = 'classification_model',\n",
    "                                      model_path = 'model.pkl', # local path\n",
    "                                      decsription = 'A classification model')\n",
    "</code>\n",
    "\n",
    "Or if you have a reference to the Run used to train the modely, you can exploit register_model:\n",
    "<code>\n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model')\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "2. The model will be deployed as a service that consists of a script which load the model and return the predictions along with the environment. So, both of them must be defined. As regards the script, create an _entry script_ as a Python file which must include _init()_, for the initialization, and _run(raw_data)_, called when new data are submitted to the service. Typically, _init_ take the model from the model registry, while _run_ generate the predictions through the model.\n",
    "\n",
    "<code>\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    predictions = model.predict(data)\n",
    "    return predictions.tolist()\n",
    "\n",
    "</code>\n",
    "\n",
    "Let's create an environment in order to run the script.\n",
    "\n",
    "<code>\n",
    "from azureml.core import Environment\n",
    "\n",
    "service_env = Environment(name='service-env')\n",
    "python_packages = ['scikit-learn', 'numpy'] # whatever packages your entry script uses\n",
    "for package in python_packages:\n",
    "    service_env.python.conda_dependencies.add_pip_package(package)\n",
    "</code>\n",
    "\n",
    "Finally, you must combine the Script and the environment by means of InferenceConfig object.\n",
    "\n",
    "<code>\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "classifier_inference_config = InferenceConfig(source_directory = 'service_files',\n",
    "                                              entry_script=\"score.py\",\n",
    "                                              environment=service_env)\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "3. Define the compute to which the service will be deployed. If an AKS cluster is used, it must be created before the deployment.\n",
    "\n",
    "<code>\n",
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "cluster_name = 'aks-cluster'\n",
    "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "production_cluster.wait_for_completion(show_output=True)\n",
    "</code>\n",
    "\n",
    "Then, the configuration should be set in the following way:\n",
    "<code>\n",
    "from azureml.core.webservices import AksWebservice\n",
    "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "</code>\n",
    "\n",
    "Moreover, it can be done in a similar way for the Azure Instances and for local services."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. _Deployment_. Let's implement the following line of codes.\n",
    "\n",
    "<code>\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model = ws.models['classification_model']\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name = 'classifier-service',\n",
    "                       models = [model],\n",
    "                       inference_config = classifier_inference_config,\n",
    "                       deployment_config = classifier_deploy_config,\n",
    "                       deployment_target = production_cluster)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Consume the service\n",
    "\n",
    "After deploying the service, it can be consumed in the following way:\n",
    "\n",
    "<code>\n",
    "import json\n",
    "\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "json_data = json.dumps({'data': x_new})\n",
    "\n",
    "response = service.run(input_data = json_data)\n",
    "\n",
    "predictions = json.loads(response)\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i], predictions[i])\n",
    "</code>\n",
    "\n",
    "If the client doesn't support SDK, a simple REST request can be done in the followinf way:\n",
    "\n",
    "<code>\n",
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "request_headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "response = requests.post(url = endpoint,\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i]), predictions[i] )\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Authentication\n",
    "\n",
    "In production you want to restrict the request to your services by means of _key_ or _token_.\n",
    "\n",
    "By default, authentication is disabled for ACI services and it is key-based for AKS. ou can optionally configure an AKS service to use token-based authentication (which is not supported for ACI services).\n",
    "\n",
    "If you are interested in an authentication ny means of keys, let's retrieve them through:\n",
    "\n",
    "<code>\n",
    "primary_key, secondary_key = service.get_keys()\n",
    "</code>\n",
    "\n",
    "Otherwise, if you'd rather tokens, use _get_tokens_.\n",
    "\n",
    "Finally, to make an authenticated call to the service's REST endpoint, run the following code:\n",
    "\n",
    "<code>\n",
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "request_headers = { \"Content-Type\":\"application/json\",\n",
    "                    \"Authorization\":\"Bearer \" + key_or_token }\n",
    "\n",
    "response = requests.post(url = endpoint,\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i]), predictions[i] )\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Troubleshooting\n",
    "\n",
    "Examine the state of the service:\n",
    "\n",
    "<code>\n",
    "from azureml.core.webservices import AksWebservice\n",
    "\n",
    "service = AksWebservice(name = 'classifier-device', workspace = ws)\n",
    "print(service.state)\n",
    "</code>\n",
    "\n",
    "If everythoing is ok, the status should be Healthy.\n",
    "\n",
    "You can also print some logs:\n",
    "<code>\n",
    "print(service.get_logs())\n",
    "</code>\n",
    "\n",
    "Moreover, sometimes debugging from a local container can be simpler:\n",
    "<code>\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=8890)\n",
    "service = Model.deploy(ws, 'test-svc', [model], inference_config, deployment_config)\n",
    "print(service.run(input_data = json_data))\n",
    "</code>\n",
    "\n",
    "Then, during the debuggind phase, your code will change some times. So, it will be reloaded:\n",
    "<code>\n",
    "service.reload()\n",
    "print(service.run(input_data = json_data))\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tutorial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment.get(workspace=ws, name='experiment_env')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Train and Register a model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: real-time-service_1672761877_d2595d80\n",
      "Web View: https://ml.azure.com/runs/real-time-service_1672761877_d2595d80?wsid=/subscriptions/d12c1b85-0a70-4232-b483-12d1ffcfc148/resourcegroups/ResourceGroupRavazzi/workspaces/ravazzil-workspace&tid=b00367e2-193a-4f48-94de-7245d45c0947\n",
      "\n",
      "Streaming user_logs/std_log.txt\n",
      "===============================\n",
      "\n",
      "/azureml-envs/azureml_fdc508b98efb05d537f12264e569dfc7/lib/python3.6/site-packages/paramiko/transport.py:33: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography.hazmat.backends import default_backend\n",
      "-c:159: FutureWarning: azureml.core: AzureML support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use AzureML will continue to work without modification, but Python 3.6 users will no longer get access to the latest AzureML features and bugfixes. We recommend that you upgrade to Python 3.7 or newer. To disable SDK V1 deprecation warning set the environment variable AZUREML_DEPRECATE_WARNING to 'False'\n",
      "/azureml-envs/azureml_fdc508b98efb05d537f12264e569dfc7/lib/python3.6/site-packages/azureml/mlflow/__init__.py:80: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n",
      "  import mlflow\n",
      "Loading Data...\n",
      "Training a logistic regression model with regularization rate of 0.01\n",
      "Accuracy: 0.774\n",
      "AUC: 0.8483441962286681\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "1 items cleaning up...\n",
      "Cleanup took 0.04692721366882324 seconds\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: real-time-service_1672761877_d2595d80\n",
      "Web View: https://ml.azure.com/runs/real-time-service_1672761877_d2595d80?wsid=/subscriptions/d12c1b85-0a70-4232-b483-12d1ffcfc148/resourcegroups/ResourceGroupRavazzi/workspaces/ravazzil-workspace&tid=b00367e2-193a-4f48-94de-7245d45c0947\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'runId': 'real-time-service_1672761877_d2595d80',\n 'target': 'ravazzil-compute',\n 'status': 'Completed',\n 'startTimeUtc': '2023-01-03T16:04:52.737694Z',\n 'endTimeUtc': '2023-01-03T16:05:09.802275Z',\n 'services': {},\n 'properties': {'_azureml.ComputeTargetType': 'amlctrain',\n  'ContentSnapshotId': '9d82149f-4de2-4943-9bba-db47c0c06b42',\n  'azureml.git.repository_uri': 'https://github.com/LuciaRavazzi/AzureML.git',\n  'mlflow.source.git.repoURL': 'https://github.com/LuciaRavazzi/AzureML.git',\n  'azureml.git.branch': 'main',\n  'mlflow.source.git.branch': 'main',\n  'azureml.git.commit': '9068858ad851f85119c972ff6d638e028207fa25',\n  'mlflow.source.git.commit': '9068858ad851f85119c972ff6d638e028207fa25',\n  'azureml.git.dirty': 'True',\n  'ProcessInfoFile': 'azureml-logs/process_info.json',\n  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n 'inputDatasets': [],\n 'outputDatasets': [],\n 'runDefinition': {'script': '2_Train.py',\n  'command': '',\n  'useAbsolutePath': False,\n  'arguments': [],\n  'sourceDirectoryDataStore': None,\n  'framework': 'Python',\n  'communicator': 'None',\n  'target': 'ravazzil-compute',\n  'dataReferences': {},\n  'data': {},\n  'outputData': {},\n  'datacaches': [],\n  'jobName': None,\n  'maxRunDurationSeconds': 2592000,\n  'nodeCount': 1,\n  'instanceTypes': [],\n  'priority': None,\n  'credentialPassthrough': False,\n  'identity': None,\n  'environment': {'name': 'experiment_env',\n   'version': 'Autosave_2023-01-03T15:16:59Z_274c0109',\n   'assetId': 'azureml://locations/francecentral/workspaces/0ee9671f-7144-4125-a665-a1b9cffe5395/environments/experiment_env/versions/Autosave_2023-01-03T15:16:59Z_274c0109',\n   'autoRebuild': True,\n   'python': {'interpreterPath': 'python',\n    'userManagedDependencies': False,\n    'condaDependencies': {'dependencies': ['python=3.6.2',\n      'scikit-learn',\n      'pandas',\n      'pip',\n      'matplotlib',\n      {'pip': ['azureml-defaults', 'azureml-mlflow']}],\n     'name': 'simple_environment'},\n    'baseCondaEnvironment': None},\n   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20221101.v1',\n    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n    'baseDockerfile': None,\n    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n    'enabled': False,\n    'arguments': []},\n   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n   'inferencingStackVersion': None},\n  'history': {'outputCollection': True,\n   'directoriesToWatch': ['logs'],\n   'enableMLflowTracking': True,\n   'snapshotProject': True},\n  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n    'spark.yarn.maxAppAttempts': '1'}},\n  'parallelTask': {'maxRetriesPerWorker': 0,\n   'workerCountPerNode': 1,\n   'terminalExitCodes': None,\n   'configuration': {}},\n  'amlCompute': {'name': None,\n   'vmSize': None,\n   'retainCluster': False,\n   'clusterMaxNodeCount': None},\n  'aiSuperComputer': {'instanceType': 'D2',\n   'imageVersion': None,\n   'location': None,\n   'aiSuperComputerStorageData': None,\n   'interactive': False,\n   'scalePolicy': None,\n   'virtualClusterArmId': None,\n   'tensorboardLogDirectory': None,\n   'sshPublicKey': None,\n   'sshPublicKeys': None,\n   'enableAzmlInt': True,\n   'priority': 'Medium',\n   'slaTier': 'Standard',\n   'userAlias': None},\n  'kubernetesCompute': {'instanceType': None},\n  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n  'mpi': {'processCountPerNode': 1},\n  'pyTorch': {'communicationBackend': 'nccl', 'processCount': None},\n  'hdi': {'yarnDeployMode': 'Cluster'},\n  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n  'exposedPorts': None,\n  'docker': {'useDocker': True,\n   'sharedVolumes': True,\n   'shmSize': '2g',\n   'arguments': []},\n  'cmk8sCompute': {'configuration': {}},\n  'commandReturnCodeConfig': {'returnCode': 'Zero',\n   'successfulReturnCodes': []},\n  'environmentVariables': {},\n  'applicationEndpoints': {},\n  'parameters': []},\n 'logFiles': {'user_logs/std_log.txt': 'https://ravazzilworksp8398449111.blob.core.windows.net/azureml/ExperimentRun/dcid.real-time-service_1672761877_d2595d80/user_logs/std_log.txt?sv=2019-07-07&sr=b&sig=%2B86k1x%2BxIQ6%2FQRGqMSSp1fqfVatT2ol5hVt%2FZRW2t6g%3D&skoid=b28ea9a2-2c96-4d13-8022-8326c2b29d17&sktid=b00367e2-193a-4f48-94de-7245d45c0947&skt=2023-01-02T17%3A28%3A56Z&ske=2023-01-04T01%3A38%3A56Z&sks=b&skv=2019-07-07&st=2023-01-03T15%3A55%3A16Z&se=2023-01-04T00%3A05%3A16Z&sp=r',\n  'system_logs/cs_capability/cs-capability.log': 'https://ravazzilworksp8398449111.blob.core.windows.net/azureml/ExperimentRun/dcid.real-time-service_1672761877_d2595d80/system_logs/cs_capability/cs-capability.log?sv=2019-07-07&sr=b&sig=GZBfFJyMGPPaOcFT60Q%2F5rXabmFu8jMXsRsAN2bLWA0%3D&skoid=b28ea9a2-2c96-4d13-8022-8326c2b29d17&sktid=b00367e2-193a-4f48-94de-7245d45c0947&skt=2023-01-03T15%3A14%3A48Z&ske=2023-01-04T23%3A24%3A48Z&sks=b&skv=2019-07-07&st=2023-01-03T15%3A55%3A17Z&se=2023-01-04T00%3A05%3A17Z&sp=r',\n  'system_logs/hosttools_capability/hosttools-capability.log': 'https://ravazzilworksp8398449111.blob.core.windows.net/azureml/ExperimentRun/dcid.real-time-service_1672761877_d2595d80/system_logs/hosttools_capability/hosttools-capability.log?sv=2019-07-07&sr=b&sig=rVNhNDmQsiBl4pY4bpu5sA8Mjd%2F%2F4gDQ9paXmjYyot0%3D&skoid=b28ea9a2-2c96-4d13-8022-8326c2b29d17&sktid=b00367e2-193a-4f48-94de-7245d45c0947&skt=2023-01-03T15%3A14%3A48Z&ske=2023-01-04T23%3A24%3A48Z&sks=b&skv=2019-07-07&st=2023-01-03T15%3A55%3A17Z&se=2023-01-04T00%3A05%3A17Z&sp=r',\n  'system_logs/lifecycler/execution-wrapper.log': 'https://ravazzilworksp8398449111.blob.core.windows.net/azureml/ExperimentRun/dcid.real-time-service_1672761877_d2595d80/system_logs/lifecycler/execution-wrapper.log?sv=2019-07-07&sr=b&sig=VKx4zKaAYkapg5YPC12bWeox2eSlTo3IxX2AXXexmNA%3D&skoid=b28ea9a2-2c96-4d13-8022-8326c2b29d17&sktid=b00367e2-193a-4f48-94de-7245d45c0947&skt=2023-01-03T15%3A14%3A48Z&ske=2023-01-04T23%3A24%3A48Z&sks=b&skv=2019-07-07&st=2023-01-03T15%3A55%3A17Z&se=2023-01-04T00%3A05%3A17Z&sp=r',\n  'system_logs/lifecycler/lifecycler.log': 'https://ravazzilworksp8398449111.blob.core.windows.net/azureml/ExperimentRun/dcid.real-time-service_1672761877_d2595d80/system_logs/lifecycler/lifecycler.log?sv=2019-07-07&sr=b&sig=gHfYQyOZ1LOyEoYKJNwpCgOwcKZHYY6iz5vlX687fuc%3D&skoid=b28ea9a2-2c96-4d13-8022-8326c2b29d17&sktid=b00367e2-193a-4f48-94de-7245d45c0947&skt=2023-01-03T15%3A14%3A48Z&ske=2023-01-04T23%3A24%3A48Z&sks=b&skv=2019-07-07&st=2023-01-03T15%3A55%3A17Z&se=2023-01-04T00%3A05%3A17Z&sp=r',\n  'system_logs/lifecycler/vm-bootstrapper.log': 'https://ravazzilworksp8398449111.blob.core.windows.net/azureml/ExperimentRun/dcid.real-time-service_1672761877_d2595d80/system_logs/lifecycler/vm-bootstrapper.log?sv=2019-07-07&sr=b&sig=28U0xkMHwh%2B%2BiXRwJLCtNGwpjD93tnd5Gc9zENsovtE%3D&skoid=b28ea9a2-2c96-4d13-8022-8326c2b29d17&sktid=b00367e2-193a-4f48-94de-7245d45c0947&skt=2023-01-03T15%3A14%3A48Z&ske=2023-01-04T23%3A24%3A48Z&sks=b&skv=2019-07-07&st=2023-01-03T15%3A55%3A17Z&se=2023-01-04T00%3A05%3A17Z&sp=r',\n  'system_logs/metrics_capability/metrics-capability.log': 'https://ravazzilworksp8398449111.blob.core.windows.net/azureml/ExperimentRun/dcid.real-time-service_1672761877_d2595d80/system_logs/metrics_capability/metrics-capability.log?sv=2019-07-07&sr=b&sig=KZI9IdCIgbly%2BcuAAohD84L8f2NSnhsj1DUrT7hZHoc%3D&skoid=b28ea9a2-2c96-4d13-8022-8326c2b29d17&sktid=b00367e2-193a-4f48-94de-7245d45c0947&skt=2023-01-03T15%3A14%3A48Z&ske=2023-01-04T23%3A24%3A48Z&sks=b&skv=2019-07-07&st=2023-01-03T15%3A55%3A17Z&se=2023-01-04T00%3A05%3A17Z&sp=r',\n  'system_logs/snapshot_capability/snapshot-capability.log': 'https://ravazzilworksp8398449111.blob.core.windows.net/azureml/ExperimentRun/dcid.real-time-service_1672761877_d2595d80/system_logs/snapshot_capability/snapshot-capability.log?sv=2019-07-07&sr=b&sig=aaYCcfqjiKKLSGZaGyN5D6PLcfo%2FMZJG4sX7c7YdIHE%3D&skoid=b28ea9a2-2c96-4d13-8022-8326c2b29d17&sktid=b00367e2-193a-4f48-94de-7245d45c0947&skt=2023-01-03T15%3A14%3A48Z&ske=2023-01-04T23%3A24%3A48Z&sks=b&skv=2019-07-07&st=2023-01-03T15%3A55%3A17Z&se=2023-01-04T00%3A05%3A17Z&sp=r'},\n 'submittedBy': 'Lucia Ravazzi'}"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "env = Environment.from_conda_specification(\"experiment_env\", \"environment.yml\")\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory='Script',\n",
    "                                script='2_Train.py',\n",
    "                                environment=env,\n",
    "                                compute_target = 'ravazzil-compute',\n",
    "                                docker_runtime_config=DockerConfiguration(use_docker=True))\n",
    "\n",
    "# Submit the experiment\n",
    "experiment = Experiment(workspace=ws, name='real-time-service')\n",
    "run = experiment.submit(config=script_config)\n",
    "run.wait_for_completion(show_output = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "Model(workspace=Workspace.create(name='ravazzil-workspace', subscription_id='d12c1b85-0a70-4232-b483-12d1ffcfc148', resource_group='ResourceGroupRavazzi'), name=diabetes_model, id=diabetes_model:6, version=6, tags={'real-time-services': '1'}, properties={'AUC': '0.8483441962286681', 'Accuracy': '0.774'})"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.register_model(model_path = 'outputs/my_diabetes_model.pkl',\n",
    "                   model_name = 'diabetes_model',\n",
    "                   tags = {'real-time-services': '1'},\n",
    "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: real-time-services\n",
      "Key: AUC, Value: 0.8483441962286681\n",
      "Key: Accuracy, Value: 0.774\n",
      "Tag: Training context\n",
      "Key: AUC, Value: 0.8483441962286681\n",
      "Key: Accuracy, Value: 0.774\n",
      "Tag: Training context\n",
      "Key: AUC, Value: 0.8731578685506741\n",
      "Key: Accuracy, Value: 0.888\n",
      "Tag: Training context\n",
      "Key: AUC, Value: 0.8756256530175631\n",
      "Key: Accuracy, Value: 0.8893333333333333\n",
      "Tag: Training context\n",
      "Key: AUC, Value: 0.8726180406985422\n",
      "Key: Accuracy, Value: 0.8856666666666667\n",
      "Tag: Training context\n",
      "Key: AUC, Value: 0.8709040250758743\n",
      "Key: Accuracy, Value: 0.8853333333333333\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "for model in Model.list(ws):\n",
    "    for tag_name in model.tags:\n",
    "        print(f'Tag: {tag_name}')\n",
    "    for key, value in model.properties.items():\n",
    "        print(f'Key: {key}, Value: {value}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# Take the last model.\n",
    "model = ws.models['diabetes_model']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "deployment_folder = './Script/diabetes_service'\n",
    "os.makedirs(deployment_folder, exist_ok=True)\n",
    "\n",
    "script_file = 'entry_script.py'\n",
    "script_path = os.path.join(deployment_folder, script_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Generate the scoring script for inferencing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Create\n",
    "The Docker container will host the web service along with all python dependencies which must be specified. Deployment can take some time because first of all, the image must be created and then, the web services must be ran."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravazzil\\Anaconda3\\envs\\azure_env\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: azureml.core.model:\n",
      "To leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \n",
      "please refer to respective documentations \n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \n",
      "For more information on migration, see https://aka.ms/acimoemigration. \n",
      "To disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2023-01-03 17:10:42+01:00 Creating Container Registry if not exists.\n",
      "2023-01-03 17:10:42+01:00 Registering the environment.\n",
      "2023-01-03 17:10:43+01:00 Use the existing image.\n",
      "2023-01-03 17:10:43+01:00 Generating deployment configuration.\n",
      "2023-01-03 17:10:44+01:00 Submitting deployment to compute.\n",
      "2023-01-03 17:10:51+01:00 Checking the status of deployment diabetes-service..\n",
      "2023-01-03 17:12:13+01:00 Checking the status of inference endpoint diabetes-service.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "service_env = Environment.get(workspace=ws, name=\"AzureML-sklearn-0.24.1-ubuntu18.04-py37-cpu-inference\")\n",
    "service_env.inferencing_stack_version=\"latest\"\n",
    "\n",
    "inference_config = InferenceConfig(source_directory=deployment_folder,\n",
    "                                   entry_script=script_file,\n",
    "                                   environment=service_env)\n",
    "# Configure the web service container.\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "\n",
    "# Deploy the model as a service.\n",
    "print('Deploying model...')\n",
    "service_name = 'diabetes-service'\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, deployment_config, overwrite=True)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03T16:12:04,327037900+00:00 - gunicorn/run \n",
      "2023-01-03T16:12:04,329259400+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:04,333536400+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-03T16:12:04,333697400+00:00 - nginx/run \n",
      "2023-01-03T16:12:04,335010200+00:00 | gunicorn/run | AzureML Container Runtime Information\n",
      "2023-01-03T16:12:04,336521300+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-03T16:12:04,338344600+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:04,346233900+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:04,352890800+00:00 | gunicorn/run | AzureML image information: sklearn-0.24.1-ubuntu18.04-py37-cpu-inference:20221024.v1\n",
      "2023-01-03T16:12:04,354272500+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:04,359602500+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:04,360987400+00:00 | gunicorn/run | PATH environment variable: /opt/miniconda/envs/amlenv/bin:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "2023-01-03T16:12:04,363663100+00:00 | gunicorn/run | PYTHONPATH environment variable: \n",
      "2023-01-03T16:12:04,368037800+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:07,062641500+00:00 | gunicorn/run | CONDAPATH environment variable: /opt/miniconda\n",
      "\n",
      "# conda environments:\n",
      "#\n",
      "base                  *  /opt/miniconda\n",
      "amlenv                   /opt/miniconda/envs/amlenv\n",
      "\n",
      "2023-01-03T16:12:07,872707700+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:07,880664100+00:00 | gunicorn/run | Pip Dependencies (before dynamic installation)\n",
      "\n",
      "azure-core==1.26.0\n",
      "azure-identity==1.11.0\n",
      "azureml-inference-server-http==0.7.6\n",
      "cachetools==5.2.0\n",
      "certifi==2022.9.24\n",
      "cffi==1.15.1\n",
      "charset-normalizer==2.1.1\n",
      "click==8.1.3\n",
      "cryptography==38.0.1\n",
      "Flask==2.1.3\n",
      "Flask-Cors==3.0.10\n",
      "google-api-core==2.10.2\n",
      "google-auth==2.13.0\n",
      "googleapis-common-protos==1.56.4\n",
      "gunicorn==20.1.0\n",
      "idna==3.4\n",
      "importlib-metadata==5.0.0\n",
      "inference-schema==1.4.2.1\n",
      "itsdangerous==2.1.2\n",
      "Jinja2==3.1.2\n",
      "joblib==1.2.0\n",
      "MarkupSafe==2.1.1\n",
      "msal==1.20.0\n",
      "msal-extensions==1.0.0\n",
      "numpy==1.21.6\n",
      "opencensus==0.11.0\n",
      "opencensus-context==0.1.3\n",
      "opencensus-ext-azure==1.1.7\n",
      "pandas==1.1.5\n",
      "portalocker==2.6.0\n",
      "protobuf==4.21.8\n",
      "psutil==5.9.3\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycparser==2.21\n",
      "PyJWT==2.6.0\n",
      "python-dateutil==2.8.2\n",
      "pytz==2022.5\n",
      "requests==2.28.1\n",
      "rsa==4.9\n",
      "scikit-learn==0.24.1\n",
      "scipy==1.7.3\n",
      "six==1.16.0\n",
      "threadpoolctl==3.1.0\n",
      "typing-extensions==4.4.0\n",
      "urllib3==1.26.12\n",
      "Werkzeug==2.2.2\n",
      "wrapt==1.12.1\n",
      "zipp==3.10.0\n",
      "\n",
      "2023-01-03T16:12:10,154909126+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:10,156710844+00:00 | gunicorn/run | Entry script directory: /var/azureml-app\n",
      "2023-01-03T16:12:10,158392260+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:10,164935123+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-03T16:12:10,167090144+00:00 | gunicorn/run | Dynamic Python Package Installation\n",
      "2023-01-03T16:12:10,168864461+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-03T16:12:10,173370705+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:10,174707218+00:00 | gunicorn/run | Dynamic Python package installation is disabled.\n",
      "2023-01-03T16:12:10,176062931+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:10,177433544+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-03T16:12:10,183625104+00:00 | gunicorn/run | Checking if the Python package azureml-inference-server-http is installed\n",
      "2023-01-03T16:12:10,185591823+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-03T16:12:10,188306849+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:11,076745786+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:11,078323302+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-03T16:12:11,079693916+00:00 | gunicorn/run | AzureML Inference Server\n",
      "2023-01-03T16:12:11,086449984+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-03T16:12:11,088050100+00:00 | gunicorn/run | \n",
      "2023-01-03T16:12:11,089812618+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.\n",
      "\n",
      "Azure ML Inferencing HTTP server v0.7.6\n",
      "\n",
      "\n",
      "Server Settings\n",
      "---------------\n",
      "Entry Script Name: /var/azureml-app/main.py\n",
      "Model Directory: /var/azureml-app/azureml-models/diabetes_model/6\n",
      "Worker Count: 1\n",
      "Worker Timeout (seconds): 300\n",
      "Server Port: 31311\n",
      "Application Insights Enabled: false\n",
      "Application Insights Key: None\n",
      "Inferencing HTTP server version: azmlinfsrv/0.7.6\n",
      "CORS for the specified origins: None\n",
      "\n",
      "\n",
      "Server Routes\n",
      "---------------\n",
      "Liveness Probe: GET   127.0.0.1:31311/\n",
      "Score:          POST  127.0.0.1:31311/score\n",
      "\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://0.0.0.0:31311 (79)\n",
      "Using worker: sync\n",
      "Booting worker with pid: 134\n",
      "Initializing logger\n",
      "2023-01-03 16:12:14,167 | root | INFO | Starting up app insights client\n",
      "logging socket not found. logging not available.\n",
      "logging socket not found. logging not available.\n",
      "2023-01-03 16:12:14,170 | root | INFO | Starting up app insight hooks\n",
      "2023-01-03 16:12:15,017 | root | INFO | Found driver script at /var/azureml-app/main.py and the score script at /var/azureml-app/diabetes_service/entry_script.py\n",
      "2023-01-03 16:12:15,017 | root | INFO | run() is not decorated. Server will invoke it with the input in JSON string.\n",
      "2023-01-03 16:12:15,018 | root | INFO | Invoking user's init function\n",
      "2023-01-03 16:12:17,196 | root | INFO | Users's init has completed successfully\n",
      "00000000-0000-0000-0000-000000000000,/opt/miniconda/envs/amlenv/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "2023-01-03 16:12:17,197 | root | INFO | Swaggers are prepared for the following versions: [2, 3].\n",
      "2023-01-03 16:12:17,197 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2023-01-03 16:12:17,201 | root | INFO | AML_FLASK_ONE_COMPATIBILITY is set. Patched Flask to ensure compatibility with Flask 1.\n",
      "2023-01-03 16:12:17,215 | root | INFO | 200\n",
      "127.0.0.1 - - [03/Jan/2023:16:12:17 +0000] \"GET /swagger.json HTTP/1.0\" 200 2265 \"-\" \"Go-http-client/1.1\"\n",
      "2023-01-03 16:12:20,882 | root | INFO | 200\n",
      "127.0.0.1 - - [03/Jan/2023:16:12:20 +0000] \"GET /swagger.json HTTP/1.0\" 200 2265 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For troubleshooting.\n",
    "print(service.get_logs())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes-service\n"
     ]
    }
   ],
   "source": [
    "for webservices in ws.webservices:\n",
    "    print(webservices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not-diabetic']\n"
     ]
    }
   ],
   "source": [
    "# Use the web service.\n",
    "\n",
    "import json\n",
    "\n",
    "x_new = [[2,180,74,24,21,23.9091702,1.488172308,22]]\n",
    "\n",
    "input_json = json.dumps({'data': x_new})\n",
    "\n",
    "predictions = service.run(input_data = input_json)\n",
    "\n",
    "predicted_classes = json.loads(predictions)\n",
    "print(predicted_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient [2, 180, 74, 24, 21, 23.9091702, 1.488172308, 22] not-diabetic\n",
      "Patient [2, 180, 74, 24, 21, 23.9091702, 1.488172308, 22] not-diabetic\n",
      "Patient [2, 180, 74, 24, 21, 23.9091702, 1.488172308, 22] not-diabetic\n"
     ]
    }
   ],
   "source": [
    "x_new = [[2,180,74,24,21,23.9091702,1.488172308,22],\n",
    "         [2,180,74,24,21,23.9091702,1.488172308,22],\n",
    "         [2,180,74,24,21,23.9091702,1.488172308,22]]\n",
    "\n",
    "input_json = json.dumps({'data': x_new})\n",
    "\n",
    "predictions = service.run(input_data = input_json)\n",
    "\n",
    "predicted_classes = json.loads(predictions)\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    print (\"Patient {}\".format(x_new[i]), predicted_classes[i] )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient [2, 180, 74, 24, 21, 23.9091702, 1.488172308, 22] not-diabetic\n",
      "Patient [2, 180, 74, 24, 21, 23.9091702, 1.488172308, 22] not-diabetic\n",
      "Patient [2, 180, 74, 24, 21, 23.9091702, 1.488172308, 22] not-diabetic\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# In production, sometimes an HTTP request is made.\n",
    "x_new = [[2,180,74,24,21,23.9091702,1.488172308,22],\n",
    "         [2,180,74,24,21,23.9091702,1.488172308,22],\n",
    "         [2,180,74,24,21,23.9091702,1.488172308,22]]\n",
    "\n",
    "input_json = json.dumps({'data': x_new})\n",
    "\n",
    "headers = { 'Content-Type':'application/json' }\n",
    "endpoint = service.scoring_uri\n",
    "\n",
    "predictions = requests.post(endpoint, input_json, headers = headers)\n",
    "\n",
    "predicted_classes = json.loads(predictions.json())\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    print (\"Patient {}\".format(x_new[i]), predicted_classes[i] )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In production, you must add a valid authentication."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service deleted.\n"
     ]
    }
   ],
   "source": [
    "service.delete()\n",
    "print ('Service deleted.')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
