{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##  Deploy real-time machine learning services with Azure Machine Learning\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "_Inferencing_ refers to the ability of a trained model to predict labels fornew data which the model has not been trained. The model is deployed as part of a service which enables applications to request some kind of inference from a small number of data observations.\n",
    "\n",
    "Azure Kubernetes Services (AKS) can be used to deploy the model as a service.\n",
    "\n",
    "AzureML uses containers as deployment mechanism by packaging the model and the code to use it as an image taht can be deployed to a container in your chosen compute target. For testing and development is ok to use your local machine, a compute instances or Azure Container Instances (ACI) but for production, you need to use some compute which meets some requirements related to scalability, performance and security.\n",
    "\n",
    "In order to do so, you should follow these tasks:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "1. After successfully training a model, it should be registered because the real-time inference service will load it when required.\n",
    "\n",
    "<code>\n",
    "from azureml.core import Model\n",
    "\n",
    "classification_model = Model.register(workpace = ws,\n",
    "                                      model_name = 'classification_model',\n",
    "                                      model_path = 'model.pkl', # local path\n",
    "                                      decsription = 'A classification model')\n",
    "</code>\n",
    "\n",
    "Or if you have a reference to the Run used to train the modely, you can exploit register_model:\n",
    "<code>\n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model')\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "2. The model will be deployed as a service that consists of a script which load the model and return the predictions along with the environment. So, both of them must be defined. As regards the script, create an _entry script_ as a Python file which must include _init()_, for the initialization, and _run(raw_data)_, called when new data are submitted to the service. Typically, _init_ take the model from the model registry, while _run_ generate the predictions through the model.\n",
    "\n",
    "<code>\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    predictions = model.predict(data)\n",
    "    return predictions.tolist()\n",
    "\n",
    "</code>\n",
    "\n",
    "Let's create an environment in order to run the script.\n",
    "\n",
    "<code>\n",
    "from azureml.core import Environment\n",
    "\n",
    "service_env = Environment(name='service-env')\n",
    "python_packages = ['scikit-learn', 'numpy'] # whatever packages your entry script uses\n",
    "for package in python_packages:\n",
    "    service_env.python.conda_dependencies.add_pip_package(package)\n",
    "</code>\n",
    "\n",
    "Finally, you must combine the Script and the environment by means of InferenceConfig object.\n",
    "\n",
    "<code>\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "classifier_inference_config = InferenceConfig(source_directory = 'service_files',\n",
    "                                              entry_script=\"score.py\",\n",
    "                                              environment=service_env)\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "3. Define the compute to which the service will be deployed. If an AKS cluster is used, it must be created before the deployment.\n",
    "\n",
    "<code>\n",
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "cluster_name = 'aks-cluster'\n",
    "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "production_cluster.wait_for_completion(show_output=True)\n",
    "</code>\n",
    "\n",
    "Then, the configuration should be set in the following way:\n",
    "<code>\n",
    "from azureml.core.webservices import AksWebservice\n",
    "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "</code>\n",
    "\n",
    "Moreover, it can be done in a similar way for the Azure Instances and for local services."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. _Deployment_. Let's implement the following line of codes.\n",
    "\n",
    "<code>\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model = ws.models['classification_model']\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name = 'classifier-service',\n",
    "                       models = [model],\n",
    "                       inference_config = classifier_inference_config,\n",
    "                       deployment_config = classifier_deploy_config,\n",
    "                       deployment_target = production_cluster)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Consume the service\n",
    "\n",
    "After deploying the service, it can be consumed in the following way:\n",
    "\n",
    "<code>\n",
    "import json\n",
    "\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "json_data = json.dumps({'data': x_new})\n",
    "\n",
    "response = service.run(input_data = json_data)\n",
    "\n",
    "predictions = json.loads(response)\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i], predictions[i])\n",
    "</code>\n",
    "\n",
    "If the client doesn't support SDK, a simple REST request can be done in the followinf way:\n",
    "\n",
    "<code>\n",
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "request_headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "response = requests.post(url = endpoint,\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i]), predictions[i] )\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Authentication\n",
    "\n",
    "In production you want to restrict the request to your services by means of _key_ or _token_.\n",
    "\n",
    "By default, authentication is disabled for ACI services and it is key-based for AKS. ou can optionally configure an AKS service to use token-based authentication (which is not supported for ACI services).\n",
    "\n",
    "If you are interested in an authentication ny means of keys, let's retrieve them through:\n",
    "\n",
    "<code>\n",
    "primary_key, secondary_key = service.get_keys()\n",
    "</code>\n",
    "\n",
    "Otherwise, if you'd rather tokens, use _get_tokens_.\n",
    "\n",
    "Finally, to make an authenticated call to the service's REST endpoint, run the following code:\n",
    "\n",
    "<code>\n",
    "import requests\n",
    "import json\n",
    "\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "request_headers = { \"Content-Type\":\"application/json\",\n",
    "                    \"Authorization\":\"Bearer \" + key_or_token }\n",
    "\n",
    "response = requests.post(url = endpoint,\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i]), predictions[i] )\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Troubleshooting\n",
    "\n",
    "Examine the state of the service:\n",
    "\n",
    "<code>\n",
    "from azureml.core.webservices import AksWebservice\n",
    "\n",
    "service = AksWebservice(name = 'classifier-device', workspace = ws)\n",
    "print(service.state)\n",
    "</code>\n",
    "\n",
    "If everythoing is ok, the status should be Healthy.\n",
    "\n",
    "You can also print some logs:\n",
    "<code>\n",
    "print(service.get_logs())\n",
    "</code>\n",
    "\n",
    "Moreover, sometimes debugging from a local container can be simpler:\n",
    "<code>\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=8890)\n",
    "service = Model.deploy(ws, 'test-svc', [model], inference_config, deployment_config)\n",
    "print(service.run(input_data = json_data))\n",
    "</code>\n",
    "\n",
    "Then, during the debuggind phase, your code will change some times. So, it will be reloaded:\n",
    "<code>\n",
    "service.reload()\n",
    "print(service.run(input_data = json_data))\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
